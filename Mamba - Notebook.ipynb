{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.10)\n",
      "Path to dataset files: C:\\Users\\Nima\\.cache\\kagglehub\\datasets\\jamiewelsh2\\rap-lyrics\\versions\\1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>lyric</th>\n",
       "      <th>next lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Fetty Wap</td>\n",
       "      <td>Trap Queen</td>\n",
       "      <td>rgf productions</td>\n",
       "      <td>remy boyz yahah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Fetty Wap</td>\n",
       "      <td>Trap Queen</td>\n",
       "      <td>remy boyz yahah</td>\n",
       "      <td>1738 ayy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Fetty Wap</td>\n",
       "      <td>Trap Queen</td>\n",
       "      <td>1738 ayy</td>\n",
       "      <td>im like hey whats up hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Fetty Wap</td>\n",
       "      <td>Trap Queen</td>\n",
       "      <td>im like hey whats up hello</td>\n",
       "      <td>seen yo pretty ass soon as you came in the door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Fetty Wap</td>\n",
       "      <td>Trap Queen</td>\n",
       "      <td>seen yo pretty ass soon as you came in the door</td>\n",
       "      <td>i just wanna chill got a sack for us to roll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     artist        song  \\\n",
       "0           0  Fetty Wap  Trap Queen   \n",
       "1           0  Fetty Wap  Trap Queen   \n",
       "2           0  Fetty Wap  Trap Queen   \n",
       "3           0  Fetty Wap  Trap Queen   \n",
       "4           0  Fetty Wap  Trap Queen   \n",
       "\n",
       "                                             lyric  \\\n",
       "0                                  rgf productions   \n",
       "1                                  remy boyz yahah   \n",
       "2                                         1738 ayy   \n",
       "3                      im like hey whats up hello    \n",
       "4  seen yo pretty ass soon as you came in the door   \n",
       "\n",
       "                                        next lyric  \n",
       "0                                  remy boyz yahah  \n",
       "1                                         1738 ayy  \n",
       "2                      im like hey whats up hello   \n",
       "3  seen yo pretty ass soon as you came in the door  \n",
       "4     i just wanna chill got a sack for us to roll  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"jamiewelsh2/rap-lyrics\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(path + \"/updated_rappers.csv\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyric</th>\n",
       "      <th>next lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rgf productions</td>\n",
       "      <td>remy boyz yahah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>remy boyz yahah</td>\n",
       "      <td>1738 ayy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1738 ayy</td>\n",
       "      <td>im like hey whats up hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im like hey whats up hello</td>\n",
       "      <td>seen yo pretty ass soon as you came in the door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seen yo pretty ass soon as you came in the door</td>\n",
       "      <td>i just wanna chill got a sack for us to roll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             lyric  \\\n",
       "0                                  rgf productions   \n",
       "1                                  remy boyz yahah   \n",
       "2                                         1738 ayy   \n",
       "3                      im like hey whats up hello    \n",
       "4  seen yo pretty ass soon as you came in the door   \n",
       "\n",
       "                                        next lyric  \n",
       "0                                  remy boyz yahah  \n",
       "1                                         1738 ayy  \n",
       "2                      im like hey whats up hello   \n",
       "3  seen yo pretty ass soon as you came in the door  \n",
       "4     i just wanna chill got a sack for us to roll  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path + \"/updated_rappers.csv\")\n",
    "    df = df[['lyric', 'next lyric']].dropna()\n",
    "    return df\n",
    "\n",
    "df = load_data(path)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# Data preprocessing: tekst tokenizen, vocabulaire bouwen en tokens maken.\n",
    "def preprocess_text(text):\n",
    "    # Verander de tekst naar kleine letters en splitst op spaties.\n",
    "    words = text.lower().split()\n",
    "    vocab = sorted(set(words))\n",
    "    word2idx = {word: i for i, word in enumerate(vocab)}\n",
    "    idx2word = {i: word for i, word in enumerate(vocab)}\n",
    "    tokens = np.array([word2idx[word] for word in words], dtype=np.int32)\n",
    "    return tokens, word2idx, idx2word\n",
    "\n",
    "# Een generator om batches te maken voor training.\n",
    "def create_batches(tokens, batch_size, seq_length):\n",
    "    n_batches = len(tokens) // (batch_size * seq_length)\n",
    "    tokens = tokens[:n_batches * batch_size * seq_length]\n",
    "    x = tokens.reshape((batch_size, -1))\n",
    "    for i in range(0, x.shape[1] - seq_length, seq_length):\n",
    "        x_batch = x[:, i:i+seq_length]\n",
    "        y_batch = x[:, i+1:i+seq_length+1]\n",
    "        yield x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deze implementatie van de Mamba class is geïnspireerd op de Mamba-paper. De recurrente update in __call__\n",
    "gebruikt het selectie-mechanisme dat ervoor zorgt dat het model inputafhankelijk kan bepalen wat relevant is.\n",
    "Dit is een vereenvoudigde versie waarin we de kernideeën van de paper toepassen:\n",
    "- Een inputembedding-laag,\n",
    "- Een selectie-mechanisme (vergelijkbaar met een gating-methode in RNNs) zoals afgeleid in Theorem 1,\n",
    "- Een outputprojectie voor next-token predictie.\n",
    "\n",
    "Daarnaast toont de generate-methode hoe je autoregressief kunt genereren, te beginnen vanaf een startwoord.\n",
    "De functie retourneert de gegenereerde tekst samen met het aantal woorden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De Mamba class gebaseerd op de Mamba-paper.\n",
    "# Deze implementatie gebruikt een selectie-mechanisme zoals beschreven in Theorem 1:\n",
    "# gₜ = σ(W_gate · xₜ + b_gate) en hₜ = (1 - gₜ) · hₜ₋₁ + gₜ · xₜ.\n",
    "# Hiermee wordt de recurrente update gemodelleerd op basis van de input, wat overeenkomt met het idee van input-afhankelijke selectie.\n",
    "class Mamba(tf.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, name=None):\n",
    "        super(Mamba, self).__init__(name=name)\n",
    "        # Embedding matrix: zet tokenindices om in vectoren.\n",
    "        self.embedding = tf.Variable(tf.random.uniform([vocab_size, embed_dim], -0.1, 0.1), name=\"embedding\")\n",
    "        # Gate parameters voor het selectie-mechanisme.\n",
    "        self.W_gate = tf.Variable(tf.random.uniform([embed_dim, embed_dim], -0.1, 0.1), name=\"W_gate\")\n",
    "        self.b_gate = tf.Variable(tf.zeros([embed_dim]), name=\"b_gate\")\n",
    "        # Output projectie naar de vocabulaire-dimensie.\n",
    "        self.W_out = tf.Variable(tf.random.uniform([embed_dim, vocab_size], -0.1, 0.1), name=\"W_out\")\n",
    "        self.b_out = tf.Variable(tf.zeros([vocab_size]), name=\"b_out\")\n",
    "        \n",
    "    def __call__(self, inputs, initial_state=None):\n",
    "        # inputs: shape (batch_size, seq_length) met token indices.\n",
    "        # initial_state: shape (batch_size, embed_dim); als None, dan wordt de state op nul gezet.\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        seq_length = tf.shape(inputs)[1]\n",
    "        x_embed = tf.nn.embedding_lookup(self.embedding, inputs)  # (batch_size, seq_length, embed_dim)\n",
    "        \n",
    "        if initial_state is None:\n",
    "            h = tf.zeros([batch_size, x_embed.shape[-1]], dtype=tf.float32)\n",
    "        else:\n",
    "            h = initial_state\n",
    "        \n",
    "        outputs = []\n",
    "        # Itereer over de tijdstappen.\n",
    "        for t in range(seq_length):\n",
    "            x_t = x_embed[:, t, :]  # (batch_size, embed_dim)\n",
    "            # Bereken de gate: gₜ = σ(W_gate·xₜ + b_gate)\n",
    "            g_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_gate) + self.b_gate)\n",
    "            # Update de hidden state volgens: hₜ = (1 - gₜ)*hₜ₋₁ + gₜ*xₜ\n",
    "            h = (1 - g_t) * h + g_t * x_t\n",
    "            # Bereken de output logits op basis van de hidden state.\n",
    "            logits = tf.matmul(h, self.W_out) + self.b_out\n",
    "            outputs.append(logits)\n",
    "        # Zet de outputs samen: (batch_size, seq_length, vocab_size)\n",
    "        logits_seq = tf.stack(outputs, axis=1)\n",
    "        return logits_seq, h\n",
    "    \n",
    "    def generate(self, start_word, word2idx, idx2word, num_words, temperature=1.0):\n",
    "        # Genereer een tekstreeks op basis van een startwoord.\n",
    "        current_word = start_word.lower()\n",
    "        generated = [current_word]\n",
    "        # Initialiseer de state.\n",
    "        h = tf.zeros([1, self.embedding.shape[1]], dtype=tf.float32)\n",
    "        token = word2idx.get(current_word, 0)\n",
    "        for _ in range(num_words - 1):\n",
    "            x_t = tf.nn.embedding_lookup(self.embedding, [token])  # (1, embed_dim)\n",
    "            g_t = tf.nn.sigmoid(tf.matmul(x_t, self.W_gate) + self.b_gate)  # (1, embed_dim)\n",
    "            h = (1 - g_t) * h + g_t * x_t\n",
    "            logits = tf.matmul(h, self.W_out) + self.b_out  # (1, vocab_size)\n",
    "            logits = logits / temperature\n",
    "            prob = tf.nn.softmax(logits)\n",
    "            # Sample de volgende token.\n",
    "            token = tf.random.categorical(tf.math.log(prob), num_samples=1)[0, 0].numpy()\n",
    "            word = idx2word[token]\n",
    "            generated.append(word)\n",
    "        return ' '.join(generated), len(generated)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data\n",
    "training_text = df['lyric'].str.cat(sep=' ')\n",
    "\n",
    "sample_text = training_text[:50000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 5.32694960512766\n",
      "Epoch 2 Loss: 3.754908847708192\n",
      "Epoch 3 Loss: 3.021244940681708\n",
      "Epoch 4 Loss: 2.5827971611444545\n",
      "Epoch 5 Loss: 2.299780155310152\n",
      "Epoch 6 Loss: 2.099918206311469\n",
      "Epoch 7 Loss: 1.9537975243144226\n",
      "Epoch 8 Loss: 1.8381781202756693\n",
      "Epoch 9 Loss: 1.755425330265434\n",
      "Epoch 10 Loss: 1.687303255960401\n",
      "Gegenereerde tekst: pretty eyes so damn robin wings bitch his inspiration put in work for that payoff\n",
      "Aantal woorden: 15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data preprocessing.\n",
    "tokens, word2idx, idx2word = preprocess_text(sample_text)\n",
    "vocab_size = len(word2idx)\n",
    "embed_dim = 32 \n",
    "\n",
    "# Maak een instantie van het Mamba-model.\n",
    "model = Mamba(vocab_size, embed_dim)\n",
    "\n",
    "# Training parameters.\n",
    "batch_size = 2\n",
    "seq_length = 5\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Eenvoudige training loop op het voorbeeldcorpus (voor demonstratiedoeleinden).\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    for x_batch, y_batch in create_batches(tokens, batch_size, seq_length):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, _ = model(x_batch)\n",
    "            loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_batch, logits=logits))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        total_loss += loss.numpy()\n",
    "        count += 1\n",
    "    print(\"Epoch\", epoch+1, \"Loss:\", total_loss/count)\n",
    "\n",
    "# Genereer tekst vanaf een startwoord.\n",
    "start_word = \"pretty\"\n",
    "generated_text, word_count = model.generate(start_word, word2idx, idx2word, num_words=15)\n",
    "print(\"Gegenereerde tekst:\", generated_text)\n",
    "print(\"Aantal woorden:\", word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
