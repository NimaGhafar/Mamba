{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.10)\n",
      "Path to dataset files: C:\\Users\\Nima\\.cache\\kagglehub\\datasets\\jamiewelsh2\\rap-lyrics\\versions\\1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>lyric</th>\n",
       "      <th>next lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Fetty Wap</td>\n",
       "      <td>Trap Queen</td>\n",
       "      <td>rgf productions</td>\n",
       "      <td>remy boyz yahah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Fetty Wap</td>\n",
       "      <td>Trap Queen</td>\n",
       "      <td>remy boyz yahah</td>\n",
       "      <td>1738 ayy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Fetty Wap</td>\n",
       "      <td>Trap Queen</td>\n",
       "      <td>1738 ayy</td>\n",
       "      <td>im like hey whats up hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Fetty Wap</td>\n",
       "      <td>Trap Queen</td>\n",
       "      <td>im like hey whats up hello</td>\n",
       "      <td>seen yo pretty ass soon as you came in the door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Fetty Wap</td>\n",
       "      <td>Trap Queen</td>\n",
       "      <td>seen yo pretty ass soon as you came in the door</td>\n",
       "      <td>i just wanna chill got a sack for us to roll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     artist        song  \\\n",
       "0           0  Fetty Wap  Trap Queen   \n",
       "1           0  Fetty Wap  Trap Queen   \n",
       "2           0  Fetty Wap  Trap Queen   \n",
       "3           0  Fetty Wap  Trap Queen   \n",
       "4           0  Fetty Wap  Trap Queen   \n",
       "\n",
       "                                             lyric  \\\n",
       "0                                  rgf productions   \n",
       "1                                  remy boyz yahah   \n",
       "2                                         1738 ayy   \n",
       "3                      im like hey whats up hello    \n",
       "4  seen yo pretty ass soon as you came in the door   \n",
       "\n",
       "                                        next lyric  \n",
       "0                                  remy boyz yahah  \n",
       "1                                         1738 ayy  \n",
       "2                      im like hey whats up hello   \n",
       "3  seen yo pretty ass soon as you came in the door  \n",
       "4     i just wanna chill got a sack for us to roll  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kagglehub\n",
    "import pandas as pd\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"jamiewelsh2/rap-lyrics\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(path + \"/updated_rappers.csv\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyric</th>\n",
       "      <th>next lyric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rgf productions</td>\n",
       "      <td>remy boyz yahah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>remy boyz yahah</td>\n",
       "      <td>1738 ayy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1738 ayy</td>\n",
       "      <td>im like hey whats up hello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im like hey whats up hello</td>\n",
       "      <td>seen yo pretty ass soon as you came in the door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seen yo pretty ass soon as you came in the door</td>\n",
       "      <td>i just wanna chill got a sack for us to roll</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             lyric  \\\n",
       "0                                  rgf productions   \n",
       "1                                  remy boyz yahah   \n",
       "2                                         1738 ayy   \n",
       "3                      im like hey whats up hello    \n",
       "4  seen yo pretty ass soon as you came in the door   \n",
       "\n",
       "                                        next lyric  \n",
       "0                                  remy boyz yahah  \n",
       "1                                         1738 ayy  \n",
       "2                      im like hey whats up hello   \n",
       "3  seen yo pretty ass soon as you came in the door  \n",
       "4     i just wanna chill got a sack for us to roll  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data\n",
    "def load_data(path):\n",
    "    df = pd.read_csv(path + \"/updated_rappers.csv\")\n",
    "    df = df[['lyric', 'next lyric']].dropna()\n",
    "    return df\n",
    "\n",
    "df = load_data(path)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Special characters weghalen\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Extra whitespace weghalen\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toepassen van clean_text op de kolommen lyric en next lyric\n",
    "def preprocess_lyrics(df):\n",
    "    df['lyric'] = df['lyric'].apply(clean_text)\n",
    "    df['next lyric'] = df['next lyric'].apply(clean_text)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "from collections import Counter\n",
    "def build_vocab(df, vocab_size=5000):\n",
    "    words = ' '.join(df['lyric']).split()\n",
    "    word_counts = Counter(words)\n",
    "    most_common = word_counts.most_common(vocab_size - 1)\n",
    "    vocab = {word: idx+1 for idx, (word, _) in enumerate(most_common)}  \n",
    "    vocab['<UNK>'] = vocab_size  # Unknown words\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def text_to_sequence(text, vocab):\n",
    "    return [vocab.get(word, vocab['<UNK>']) for word in text.split()]\n",
    "\n",
    "# How do I make train and test set the same size using bag of words. (n.d.).\n",
    "# Stack Overflow. https://stackoverflow.com/questions/67494914/how-do-i-make-train-and-test-set-the-same-size-using-bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data naar numerieke waarden omzetten\n",
    "def prepare_data(filepath, vocab_size=5000):\n",
    "    df = load_data(filepath)\n",
    "    df = preprocess_lyrics(df)\n",
    "    vocab = build_vocab(df, vocab_size)\n",
    "    df['lyric_seq'] = df['lyric'].apply(lambda x: text_to_sequence(x, vocab))\n",
    "    df['next_lyric_seq'] = df['next lyric'].apply(lambda x: text_to_sequence(x, vocab))\n",
    "    return df, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, vocab = prepare_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyric</th>\n",
       "      <th>next lyric</th>\n",
       "      <th>lyric_seq</th>\n",
       "      <th>next_lyric_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rgf productions</td>\n",
       "      <td>remy boyz yahah</td>\n",
       "      <td>[5000, 5000]</td>\n",
       "      <td>[2462, 3022, 5000]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>remy boyz yahah</td>\n",
       "      <td>ayy</td>\n",
       "      <td>[2462, 3022, 5000]</td>\n",
       "      <td>[321]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ayy</td>\n",
       "      <td>im like hey whats up hello</td>\n",
       "      <td>[321]</td>\n",
       "      <td>[11, 14, 227, 193, 18, 929]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im like hey whats up hello</td>\n",
       "      <td>seen yo pretty ass soon as you came in the door</td>\n",
       "      <td>[11, 14, 227, 193, 18, 929]</td>\n",
       "      <td>[254, 98, 568, 124, 622, 84, 3, 199, 9, 1, 418]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seen yo pretty ass soon as you came in the door</td>\n",
       "      <td>i just wanna chill got a sack for us to roll</td>\n",
       "      <td>[254, 98, 568, 124, 622, 84, 3, 199, 9, 1, 418]</td>\n",
       "      <td>[2, 36, 72, 757, 21, 4, 1583, 19, 122, 6, 255]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             lyric  \\\n",
       "0                                  rgf productions   \n",
       "1                                  remy boyz yahah   \n",
       "2                                              ayy   \n",
       "3                       im like hey whats up hello   \n",
       "4  seen yo pretty ass soon as you came in the door   \n",
       "\n",
       "                                        next lyric  \\\n",
       "0                                  remy boyz yahah   \n",
       "1                                              ayy   \n",
       "2                       im like hey whats up hello   \n",
       "3  seen yo pretty ass soon as you came in the door   \n",
       "4     i just wanna chill got a sack for us to roll   \n",
       "\n",
       "                                         lyric_seq  \\\n",
       "0                                     [5000, 5000]   \n",
       "1                               [2462, 3022, 5000]   \n",
       "2                                            [321]   \n",
       "3                      [11, 14, 227, 193, 18, 929]   \n",
       "4  [254, 98, 568, 124, 622, 84, 3, 199, 9, 1, 418]   \n",
       "\n",
       "                                    next_lyric_seq  \n",
       "0                               [2462, 3022, 5000]  \n",
       "1                                            [321]  \n",
       "2                      [11, 14, 227, 193, 18, 929]  \n",
       "3  [254, 98, 568, 124, 622, 84, 3, 199, 9, 1, 418]  \n",
       "4   [2, 36, 72, 757, 21, 4, 1583, 19, 122, 6, 255]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier is een uitgebreide, gestructureerde uitleg van de **Mamba-class**, inclusief duidelijke referenties naar de paper. Je kunt dit direct in je Markdown-document plaatsen.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Implementatie en uitleg van het Mamba-model**  \n",
    "\n",
    "De **Mamba-architectuur** is een nieuw type sequentiemodel dat gebruik maakt van **State Space Models (SSMs)**, in plaats van zelf-attentie zoals in Transformers. In tegenstelling tot klassieke SSMs, die een vast geheugen hebben, introduceert Mamba een **selectief mechanisme** dat het mogelijk maakt om irrelevante informatie te filteren en belangrijke tokens vast te houden. Dit maakt het model efficiënter en beter in het verwerken van lange sequenties.  \n",
    "\n",
    "Onze implementatie is gebaseerd op de principes uit de paper *Mamba: Linear-Time Sequence Modeling with Selective State Spaces* (Gu & Dao, 2024) en bouwt stap voor stap een eenvoudige versie van het model op.  \n",
    "\n",
    "---\n",
    "\n",
    "### **1. De Basis: State Space Models (SSMs)**  \n",
    "\n",
    "In de paper wordt uitgelegd dat **State Space Models (SSMs)** werken met een verborgen toestand (\\( h_t \\)) die bij elke nieuwe token wordt bijgewerkt volgens de volgende formule:  \n",
    "\n",
    "\\[\n",
    "h_t = A h_{t-1} + B x_t\n",
    "\\]\n",
    "\n",
    "en de uiteindelijke output wordt berekend als:\n",
    "\n",
    "\\[\n",
    "y_t = C h_t\n",
    "\\]\n",
    "\n",
    "Dit betekent dat de toestand van het model op elk moment afhangt van de vorige toestand (\\( h_{t-1} \\)) en de huidige input (\\( x_t \\)). Dit concept komt uit de klassieke **lineaire recursieve netwerken** en wordt gebruikt om lange-afstandsafhankelijkheden in tekst te modelleren.  \n",
    "\n",
    "**Referentie:** Zie sectie *State Space Models* (pagina 3) waar deze basisformules worden geïntroduceerd:  \n",
    "\n",
    "*\"S4 models are defined with four parameters (Δ, A, B, C), which define a sequence-to-sequence transformation in two stages.\"*  \n",
    "\n",
    "Om dit in onze code om te zetten, gebruiken we een eenvoudige implementatie waarbij een verborgen toestand (\\( h_t \\)) wordt bijgehouden en geüpdatet bij elke nieuwe token:  \n",
    "\n",
    "```python\n",
    "self.hidden_state = np.tanh(np.dot(self.W, self.hidden_state) + np.dot(self.U, x) + self.b)\n",
    "```\n",
    "\n",
    "Hierbij:\n",
    "- **\\( W \\)** is de matrix die de toestand bijwerkt op basis van de vorige toestand.\n",
    "- **\\( U \\)** transformeert de invoer naar een vector die kan worden toegevoegd aan de verborgen toestand.\n",
    "- **\\( b \\)** is een bias-term.\n",
    "- **\\( \\tanh \\)** zorgt ervoor dat de waarden binnen een bepaald bereik blijven en voorkomt dat de toestand explodeert.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Selectieve Informatieopslag: De Verbetering van Mamba**  \n",
    "\n",
    "Een van de kernproblemen van eerdere SSMs was dat ze **tijd-invariant** waren: de overgangsmatrix (\\( A \\)) veranderde niet afhankelijk van de invoer. Dit betekende dat ze **geen content-afhankelijke beslissingen konden nemen**, zoals wanneer een woord belangrijk is om te onthouden.  \n",
    "\n",
    "Mamba lost dit op door de **SSM-parameters input-afhankelijk te maken**. Dit betekent dat de manier waarop het model informatie doorgeeft, varieert afhankelijk van de invoer die het krijgt. Hierdoor kan Mamba:  \n",
    "- Relevante tokens opslaan en irrelevante negeren.  \n",
    "- Informatie veel efficiënter doorgeven.  \n",
    "\n",
    "**Referentie:** Zie sectie *Selection Mechanism* (pagina 5), waar wordt uitgelegd hoe Mamba een selectiemechanisme toevoegt aan de klassieke SSM-structuur:  \n",
    "\n",
    "*\"Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input.\"*  \n",
    "\n",
    "Onze implementatie vertaalt dit concept als volgt:  \n",
    "\n",
    "```python\n",
    "for token in input_seq:\n",
    "    x = np.zeros(self.vocab_size)\n",
    "    x[token] = 1  # One-hot encoding van de input\n",
    "    self.hidden_state = np.tanh(np.dot(self.W, self.hidden_state) + np.dot(self.U, x) + self.b)\n",
    "```\n",
    "\n",
    "Hier zorgt de **one-hot encoding** ervoor dat elk woord wordt omgezet in een vector, en de overgangsformule past zich dynamisch aan op basis van deze invoer.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Autoregressieve Generatie**  \n",
    "\n",
    "Omdat Mamba ontworpen is als een **autoregressief model**, kan het worden gebruikt om tekst te genereren. Dit betekent dat we het model een beginzin geven en het vervolgens **stap voor stap nieuwe tokens laat voorspellen** op basis van de verborgen toestand.  \n",
    "\n",
    "De paper beschrijft dat Mamba **geen cache van eerdere tokens nodig heeft**, in tegenstelling tot Transformers. Dit maakt het model veel efficiënter in inference.  \n",
    "\n",
    "**Referentie:** Zie sectie *Inference and Scaling* (pagina 2):  \n",
    "\n",
    "*\"Unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements.\"*  \n",
    "\n",
    "In onze implementatie gebeurt dat zo:  \n",
    "\n",
    "```python\n",
    "def generate(self, start_seq, length=10):\n",
    "    generated = list(start_seq)\n",
    "    for _ in range(length):\n",
    "        state = self.forward(generated[-1:])\n",
    "        next_token = np.argmax(state)  # Kies de meest waarschijnlijke volgende token\n",
    "        generated.append(next_token)\n",
    "    return generated\n",
    "```\n",
    "\n",
    "Hierbij:\n",
    "- **We starten met een beginzin** (`start_seq`).\n",
    "- **Elke nieuwe token wordt gegenereerd door het model** op basis van de vorige token.\n",
    "- **De output wordt gegenereerd met `np.argmax(state)`**, wat de meest waarschijnlijke token selecteert.\n",
    "\n",
    "Dit betekent dat als we bijvoorbeeld de input `\"ayy\"` geven, het model mogelijk `\"im like hey whats up\"` genereert als de volgende woorden.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mamba model implementeren\n",
    "class Mamba:\n",
    "    def __init__(self, vocab_size, hidden_size=128):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.W = np.random.randn(hidden_size, hidden_size) * 0.1\n",
    "        self.U = np.random.randn(hidden_size, vocab_size) * 0.1\n",
    "        self.b = np.zeros(hidden_size)\n",
    "        self.hidden_state = np.zeros(hidden_size)\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        for token in input_seq:\n",
    "            x = np.zeros(self.vocab_size)\n",
    "            x[token] = 1  # One-hot encoding\n",
    "            self.hidden_state = np.tanh(np.dot(self.W, self.hidden_state) + np.dot(self.U, x) + self.b)\n",
    "        return self.hidden_state\n",
    "    \n",
    "    def generate(self, start_seq, length=10):\n",
    "        generated = list(start_seq)\n",
    "        for _ in range(length):\n",
    "            state = self.forward(generated[-1:])\n",
    "            next_token = np.argmax(state)  # Simplified selection\n",
    "            generated.append(next_token)\n",
    "        return generated\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "just go need then bitch me so ill yeah when these baby never here never feel baby my just my hit take never still baby never me have still you shit\n"
     ]
    }
   ],
   "source": [
    "# example usage\n",
    "model = Mamba(vocab_size=len(vocab))\n",
    "generated_seq = model.generate([vocab['nigga']], length=30)\n",
    "\n",
    "# Convert sequence back to text\n",
    "generated_text = ' '.join([list(vocab.keys())[idx] for idx in generated_seq])\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 544053.5890\n",
      "Epoch 2/40, Loss: 553453.7766\n",
      "Epoch 3/40, Loss: 553493.4035\n",
      "Epoch 4/40, Loss: 553521.6863\n",
      "Epoch 5/40, Loss: 553540.9715\n",
      "Epoch 6/40, Loss: 553553.1132\n",
      "Epoch 7/40, Loss: 553559.4988\n",
      "Epoch 8/40, Loss: 553562.1338\n",
      "Epoch 9/40, Loss: 553562.8328\n",
      "Epoch 10/40, Loss: 553562.9410\n",
      "Epoch 11/40, Loss: 553562.9541\n",
      "Epoch 12/40, Loss: 553562.9557\n",
      "Epoch 13/40, Loss: 553562.9559\n",
      "Epoch 14/40, Loss: 553562.9560\n",
      "Epoch 15/40, Loss: 553562.9560\n",
      "Epoch 16/40, Loss: 553562.9560\n",
      "Epoch 17/40, Loss: 553562.9560\n",
      "Epoch 18/40, Loss: 553562.9561\n",
      "Epoch 19/40, Loss: 553562.9561\n",
      "Epoch 20/40, Loss: 553562.9561\n",
      "Epoch 21/40, Loss: 553562.9561\n",
      "Epoch 22/40, Loss: 553562.9561\n",
      "Epoch 23/40, Loss: 553562.9561\n",
      "Epoch 24/40, Loss: 553562.9561\n",
      "Epoch 25/40, Loss: 553562.9561\n",
      "Epoch 26/40, Loss: 553562.9561\n",
      "Epoch 27/40, Loss: 553562.9561\n",
      "Epoch 28/40, Loss: 553562.9561\n",
      "Epoch 29/40, Loss: 553562.9561\n",
      "Epoch 30/40, Loss: 553562.9562\n",
      "Epoch 31/40, Loss: 553562.9562\n",
      "Epoch 32/40, Loss: 553562.9562\n",
      "Epoch 33/40, Loss: 553562.9562\n",
      "Epoch 34/40, Loss: 553562.9562\n",
      "Epoch 35/40, Loss: 553562.9562\n",
      "Epoch 36/40, Loss: 553562.9562\n",
      "Epoch 37/40, Loss: 553562.9562\n",
      "Epoch 38/40, Loss: 553562.9562\n",
      "Epoch 39/40, Loss: 553562.9562\n",
      "Epoch 40/40, Loss: 553562.9562\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "lyrics = df['lyric'].dropna().tolist()[:20000]  # Gebruik alleen de eerste 10.000 rijen voor test\n",
    "\n",
    "# Eenvoudige tokenizer (woord naar ID)\n",
    "def build_vocab(lyrics):\n",
    "    vocab = {word: idx for idx, word in enumerate(set(\" \".join(lyrics).split()))}\n",
    "    vocab[\"<UNK>\"] = len(vocab)  # Onbekende woorden\n",
    "    return vocab\n",
    "\n",
    "# Omgekeerde vocab (ID naar woord)\n",
    "def build_reverse_vocab(vocab):\n",
    "    return {idx: word for word, idx in vocab.items()}\n",
    "\n",
    "# Converteer tekst naar ID's\n",
    "def text_to_sequence(text, vocab):\n",
    "    return [vocab.get(word, vocab[\"<UNK>\"]) for word in text.split()]\n",
    "\n",
    "# Converteer ID's terug naar tekst\n",
    "def sequence_to_text(sequence, reverse_vocab):\n",
    "    return \" \".join([reverse_vocab[idx] for idx in sequence])\n",
    "\n",
    "# Geoptimaliseerd generatief Mamba-achtig model\n",
    "class GenerativeMamba:\n",
    "    def __init__(self, vocab_size, hidden_size=128, learning_rate=0.01, batch_size=4, temperature=1.0):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # Gewichtsmatrices voor vectorverwerking\n",
    "        self.W = np.random.randn(hidden_size, vocab_size) * 0.01  # Input -> Hidden\n",
    "        self.U = np.random.randn(hidden_size, hidden_size) * 0.01  # Hidden -> Hidden\n",
    "        self.V = np.random.randn(vocab_size, hidden_size) * 0.01  # Hidden -> Output\n",
    "        \n",
    "        # Initialiseer hidden state\n",
    "        self.h = np.zeros((self.batch_size, hidden_size))\n",
    "    \n",
    "    def step(self, X_batch):\n",
    "        H = np.tanh(np.dot(X_batch, self.W.T) + np.dot(self.h, self.U.T))  # State update voor batch\n",
    "        output = np.dot(H, self.V.T)  # Output berekenen\n",
    "        self.h = H  # Hidden state update\n",
    "        return output\n",
    "    \n",
    "    def train(self, sequences, epochs=50):\n",
    "        num_samples = len(sequences)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            \n",
    "            for i in range(0, num_samples, self.batch_size):\n",
    "                batch = sequences[i:i + self.batch_size]\n",
    "                batch_size_actual = len(batch)\n",
    "                X_batch = np.zeros((batch_size_actual, len(vocab)))\n",
    "                Y_batch = np.zeros((batch_size_actual, len(vocab)))\n",
    "                \n",
    "                for j, seq in enumerate(batch):\n",
    "                    if len(seq) > 1:\n",
    "                        X_batch[j, seq[:-1]] = 1  # One-hot input\n",
    "                        Y_batch[j, seq[1:]] = 1  # Verwachte output\n",
    "                \n",
    "                logits = self.step(X_batch)\n",
    "                logits = logits - np.max(logits, axis=1, keepdims=True)  # **Numerieke stabiliteit fix**\n",
    "                probs = np.exp(logits / self.temperature)\n",
    "                probs /= np.sum(probs, axis=1, keepdims=True)  # **Zorg dat som = 1**\n",
    "                \n",
    "                if np.any(np.isnan(probs)) or np.any(np.isinf(probs)):\n",
    "                    print(\"Waarschuwing: ongeldige waarschijnlijkheden gedetecteerd, standaard fallback gebruikt.\")\n",
    "                    probs = np.nan_to_num(probs, nan=1e-9)  # Vervang NaN's met kleine waarde\n",
    "                \n",
    "                loss = -np.sum(Y_batch * np.log(probs + 1e-9)) / batch_size_actual  # Gemiddelde loss\n",
    "                total_loss += loss\n",
    "                \n",
    "                # Backpropagation\n",
    "                dL_dV = np.dot((probs - Y_batch).T, self.h)  # Correcte vorm\n",
    "                dL_dh = np.dot((probs - Y_batch), self.V) * (1 - self.h ** 2)\n",
    "                dL_dW = np.dot(dL_dh.T, X_batch)\n",
    "                dL_dU = np.dot(dL_dh.T, self.h)\n",
    "                \n",
    "                # Update gewichten\n",
    "                self.V -= self.learning_rate * dL_dV\n",
    "                self.W -= self.learning_rate * dL_dW\n",
    "                self.U -= self.learning_rate * dL_dU\n",
    "            \n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "    \n",
    "    def generate(self, start_seq, vocab, reverse_vocab, length=10):\n",
    "        seq = text_to_sequence(start_seq, vocab)\n",
    "        generated = seq[:]\n",
    "        \n",
    "        for _ in range(length):\n",
    "            x = np.zeros(len(vocab))\n",
    "            x[seq[-1]] = 1  # One-hot encoding\n",
    "            \n",
    "            logits = self.step(x.reshape(1, -1))[0]\n",
    "            logits = logits - np.max(logits)  # **Numerieke stabiliteit fix**\n",
    "            probs = np.exp(logits / self.temperature)\n",
    "            probs /= np.sum(probs)  # **Softmax correctie**\n",
    "            \n",
    "            if np.any(np.isnan(probs)) or np.any(np.isinf(probs)):\n",
    "                print(\"Waarschuwing: ongeldige waarschijnlijkheden gedetecteerd, fallback naar argmax.\")\n",
    "                next_idx = np.argmax(logits)  # Gebruik argmax als fallback\n",
    "            else:\n",
    "                next_idx = np.random.choice(len(vocab), p=probs)  # **Correcte sampling**\n",
    "                \n",
    "            generated.append(next_idx)\n",
    "            seq.append(next_idx)\n",
    "            \n",
    "        return sequence_to_text(generated, reverse_vocab)\n",
    "\n",
    "# Vocab maken\n",
    "vocab = build_vocab(lyrics)\n",
    "reverse_vocab = build_reverse_vocab(vocab)\n",
    "\n",
    "# Convert dataset naar sequenties\n",
    "sequences = [text_to_sequence(line, vocab) for line in lyrics]\n",
    "\n",
    "# Model maken en trainen\n",
    "generator = GenerativeMamba(vocab_size=len(vocab), batch_size=4, temperature=0.8)\n",
    "generator.train(sequences, epochs=40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: she got to i on that i the me to the the the the in that the that on my the to the the the that you in like my the the you in the like the me in the my that a that like a you the the you to like the the the in my i the that the the i the my in it in the on my the my that to the im to my to a a it you to the my the i to that in i me a the it me i the the the\n"
     ]
    }
   ],
   "source": [
    "# Genereer tekst\n",
    "start = \"she got\"\n",
    "print(\"Generated:\", generator.generate(start, vocab, reverse_vocab, length=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
